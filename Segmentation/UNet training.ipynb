{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "import functions as f\n",
    "import tensorflow as tf\n",
    "from patchify import patchify\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Rescaling\n",
    "from matplotlib.patches import Rectangle\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical, image_dataset_from_directory, plot_model\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "NUM_ClASSES = 10\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "INPUT_SIZE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "DIR_DATA = r\"E:\\Segmentation\\datasets\\FloodNet-Supervised_v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data: {len(os.listdir(os.path.join(DIR_DATA, 'train', 'train-org-img')))}\")\n",
    "print(f\"Validation data: {len(os.listdir(os.path.join(DIR_DATA, 'val', 'val-org-img')))}\")\n",
    "print(f\"Test data: {len(os.listdir(os.path.join(DIR_DATA, 'test', 'test-org-img')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_masks(data_dir, split=\"train\"):\n",
    "    image_list = []\n",
    "    mask_list = []\n",
    "\n",
    "    for file in tqdm(os.listdir(os.path.join(data_dir, f\"{split}/{split}-org-img\"))):\n",
    "        if file.endswith(\".jpg\"): \n",
    "            image_path = os.path.join(data_dir,  f\"{split}/{split}-org-img\", file)\n",
    "            mask_path = (os.path.join(data_dir,  f\"{split}/{split}-label-img\", file[:-4] + \"_lab.png\"))\n",
    "\n",
    "            image = cv2.imread(image_path)\n",
    "            mask = cv2.imread(mask_path)\n",
    "\n",
    "            image_list.append(np.asarray(image))\n",
    "            mask_list.append(np.asarray(mask))\n",
    "\n",
    "    return image_list, mask_list\n",
    "\n",
    "\n",
    "X_val, Y_val = load_images_and_masks(f'{DIR_DATA}/', \"val\")\n",
    "print(f\"Validation data: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the source of color map used:\n",
    "Source: https://github.com/farshadsafavi/BiseNetV2/blob/main/colors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map= {'Background':0, 'Building-flooded':1, 'Building-non-flooded':2, 'Road-flooded':3, 'Road-non-flooded':4, 'Water':5, 'Tree':6, 'Vehicle':7, 'Pool':8, 'Grass':9}\n",
    "\n",
    "color_map = {\n",
    "    \"Background\": [0, 0, 0],\n",
    "    \"Building-flooded\": [255, 0, 0],\n",
    "    \"Building-non-flooded\": [0, 255, 0],\n",
    "    \"Road-flooded\": [0, 255, 120],\n",
    "    \"Road-non-flooded\": [0, 0, 255],\n",
    "    \"Water\": [255, 0, 255],\n",
    "    \"Tree\": [70, 70, 70],\n",
    "    \"Vehicle\": [102, 102, 156],\n",
    "    \"Pool\": [190, 153, 153],\n",
    "    \"Grass\": [180, 165, 180]\n",
    "}\n",
    "\n",
    "handles = [\n",
    "    Rectangle((0, 0), 1, 1, color=np.array(c)/255) for n, c in color_map.items()\n",
    "]\n",
    "labels = [n for n, c in color_map.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_with_masks(images, masks, class_map, color_map):\n",
    "    for i in range(5):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Image')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        mask_colored = np.zeros_like(images[i], dtype=np.uint8)\n",
    "        for class_name, class_idx in class_map.items():\n",
    "            color = color_map[class_name]\n",
    "            mask_indices = np.where(masks[i] == class_idx)\n",
    "            mask_colored[mask_indices[0], mask_indices[1], :] = color\n",
    "        plt.imshow(cv2.cvtColor(mask_colored, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Colored Mask')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(masks[i], cmap='viridis')\n",
    "        plt.title('Original Mask')\n",
    "\n",
    "        plt.legend(handles, labels, bbox_to_anchor =(-0.8,-0.5), loc='lower center', ncol=5)\n",
    "        plt.show()\n",
    "\n",
    "display_images_with_masks(X_val[10:], Y_val[10:], class_map, color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_generator(image_path, mask_path, batch_size, image_size):\n",
    "    image_generator = image_dataset_from_directory(\n",
    "        directory=image_path,\n",
    "        labels=None,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(image_size, image_size),\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        label_mode=None,\n",
    "        color_mode=\"rgb\"\n",
    "    )\n",
    "\n",
    "    mask_generator = image_dataset_from_directory(\n",
    "        directory=mask_path,\n",
    "        labels=None,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(image_size, image_size),\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        label_mode=None,\n",
    "        color_mode=\"grayscale\"\n",
    "    )\n",
    "\n",
    "    combined_generator = tf.data.Dataset.zip((image_generator, mask_generator))\n",
    "\n",
    "    return combined_generator\n",
    "\n",
    "# Training set\n",
    "train_generator = load_combined_generator(\n",
    "    f'{DIR_DATA}\\\\train\\\\train-org-img',\n",
    "    f'{DIR_DATA}\\\\train\\\\train-label-img',\n",
    "    BATCH_SIZE,\n",
    "    IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# Validation set\n",
    "val_generator = load_combined_generator(\n",
    "    f'{DIR_DATA}\\\\val\\\\val-org-img',\n",
    "    f'{DIR_DATA}\\\\val\\\\val-label-img',\n",
    "    BATCH_SIZE,\n",
    "    IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# Test set\n",
    "test_generator = load_combined_generator(\n",
    "    f'{DIR_DATA}\\\\test\\\\test-org-img',\n",
    "    f'{DIR_DATA}\\\\test\\\\test-label-img',\n",
    "    BATCH_SIZE,\n",
    "    IMAGE_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(img_shape, height=4000, width=3000, channel=3, n_classes=10):\n",
    "    # input layer shape is equal to patch image size\n",
    "    inputs = Input(shape=img_shape)\n",
    "\n",
    "    # rescale images from (0, 255) to (0, 1)\n",
    "    rescale = Rescaling(scale=1. / 255, input_shape=(height, width, channel))(inputs)\n",
    "    previous_block_activation = rescale  # Set aside residual\n",
    "\n",
    "    contraction = {}\n",
    "    # # Contraction path: Blocks 1 through 5 are identical apart from the feature depth\n",
    "    for f in [16, 32, 64, 128]:\n",
    "        x = Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(\n",
    "            previous_block_activation)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        contraction[f'conv{f}'] = x\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    c5 = Conv2D(160, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(\n",
    "        previous_block_activation)\n",
    "    c5 = Dropout(0.2)(c5)\n",
    "    c5 = Conv2D(160, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    previous_block_activation = c5\n",
    "\n",
    "    # Expansive path: Second half of the network: upsampling inputs\n",
    "    for f in reversed([16, 32, 64, 128]):\n",
    "        x = Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation)\n",
    "        x = concatenate([x, contraction[f'conv{f}']])\n",
    "        x = Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    outputs = Conv2D(filters=n_classes, kernel_size=(1, 1), activation=\"softmax\")(previous_block_activation)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = f.build_unet(img_shape=INPUT_SIZE, n_classes=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelcheckpoint\n",
    "checkpointer = ModelCheckpoint(\"/models/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [\n",
    "      EarlyStopping(patience=2, monitor='val_loss'),\n",
    "        TensorBoard(log_dir='logs')]\n",
    "\n",
    "def iou_coefficient(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1, 2, 3])\n",
    "    union = K.sum(y_true, [1, 2, 3]) + K.sum(y_pred, [1, 2, 3]) - intersection\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "    return iou\n",
    "\n",
    "# jaccard similarity: the size of the intersection divided by the size of the union of two sets\n",
    "def jaccard_index(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\", iou_coefficient, jaccard_index])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(train_generator, validation_data=val_generator, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
