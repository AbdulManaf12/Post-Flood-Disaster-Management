{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.13.1+cu116', '4.38.2', '0.4.1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import evaluate\n",
    "\n",
    "torch.__version__, transformers.__version__, evaluate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n",
      "Mask2FormerForUniversalSegmentation(\n",
      "  (model): Mask2FormerModel(\n",
      "    (pixel_level_module): Mask2FormerPixelLevelModule(\n",
      "      (encoder): SwinBackbone(\n",
      "        (embeddings): SwinEmbeddings(\n",
      "          (patch_embeddings): SwinPatchEmbeddings(\n",
      "            (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "          )\n",
      "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (encoder): SwinEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): SwinStage(\n",
      "              (blocks): ModuleList(\n",
      "                (0): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (downsample): SwinPatchMerging(\n",
      "                (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinStage(\n",
      "              (blocks): ModuleList(\n",
      "                (0): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (downsample): SwinPatchMerging(\n",
      "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinStage(\n",
      "              (blocks): ModuleList(\n",
      "                (0): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (5): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (downsample): SwinPatchMerging(\n",
      "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinStage(\n",
      "              (blocks): ModuleList(\n",
      "                (0): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): SwinLayer(\n",
      "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attention): SwinAttention(\n",
      "                    (self): SwinSelfAttention(\n",
      "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                    (output): SwinSelfOutput(\n",
      "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                      (dropout): Dropout(p=0.0, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                  (drop_path): SwinDropPath(p=0.3)\n",
      "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (intermediate): SwinIntermediate(\n",
      "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                    (intermediate_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (output): SwinOutput(\n",
      "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (hidden_states_norms): ModuleDict(\n",
      "          (stage1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (stage2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (stage3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (stage4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder): Mask2FormerPixelDecoder(\n",
      "        (position_embedding): Mask2FormerSinePositionEmbedding()\n",
      "        (input_projections): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (encoder): Mask2FormerPixelDecoderEncoderOnly(\n",
      "          (layers): ModuleList(\n",
      "            (0): Mask2FormerPixelDecoderEncoderLayer(\n",
      "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): Mask2FormerPixelDecoderEncoderLayer(\n",
      "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): Mask2FormerPixelDecoderEncoderLayer(\n",
      "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): Mask2FormerPixelDecoderEncoderLayer(\n",
      "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): Mask2FormerPixelDecoderEncoderLayer(\n",
      "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): Mask2FormerPixelDecoderEncoderLayer(\n",
      "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (adapter_1): Sequential(\n",
      "          (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (layer_1): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transformer_module): Mask2FormerTransformerModule(\n",
      "      (position_embedder): Mask2FormerSinePositionEmbedding()\n",
      "      (queries_embedder): Embedding(100, 256)\n",
      "      (queries_features): Embedding(100, 256)\n",
      "      (decoder): Mask2FormerMaskedAttentionDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): Mask2FormerMaskedAttentionDecoderLayer(\n",
      "            (self_attn): Mask2FormerAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mask_predictor): Mask2FormerMaskPredictor(\n",
      "          (mask_embedder): Mask2FormerMLPPredictionHead(\n",
      "            (0): Mask2FormerPredictionBlock(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (1): Mask2FormerPredictionBlock(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): ReLU()\n",
      "            )\n",
      "            (2): Mask2FormerPredictionBlock(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (1): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (level_embed): Embedding(3, 256)\n",
      "    )\n",
      "  )\n",
      "  (class_predictor): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (criterion): Mask2FormerLoss(\n",
      "    (matcher): Mask2FormerHungarianMatcher()\n",
      "  )\n",
      ")\n",
      "47,403,396 total parameters.\n",
      "47,403,396 training parameters.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "EPOCH: 1\n",
      "Training\n",
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n",
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n",
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n",
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n",
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n",
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n",
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n",
      "Namespace(epochs=50, lr=0.0001, batch=4, imgsz=[512, 512], backbone='facebook/mask2former-swin-tiny-ade-semantic', scheduler=False, scheduler_epochs=[50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-tiny-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([10, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|                    | 0/362 [00:00<?, ?it/s]c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\features\\image.py:341: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "\n",
      "  0%|                    | 1/362 [01:25<8:31:37, 85.03s/it]\n",
      "  0%|                    | 1/362 [01:28<8:50:45, 88.21s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Post-Flood-Disaster-Management\\Segmentation\\Segmentation-2\\Mask2Former\\train.py\", line 123, in <module>\n",
      "    train_epoch_loss, train_epoch_miou = train(\n",
      "  File \"e:\\Post-Flood-Disaster-Management\\Segmentation\\Segmentation-2\\Mask2Former\\engine.py\", line 36, in train\n",
      "    outputs = model(\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mask2former\\modeling_mask2former.py\", line 2493, in forward\n",
      "    outputs = self.model(\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mask2former\\modeling_mask2former.py\", line 2268, in forward\n",
      "    transformer_module_output = self.transformer_module(\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mask2former\\modeling_mask2former.py\", line 2077, in forward\n",
      "    decoder_output = self.decoder(\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mask2former\\modeling_mask2former.py\", line 1881, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mask2former\\modeling_mask2former.py\", line 1743, in forward\n",
      "    outputs = self.forward_post(\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mask2former\\modeling_mask2former.py\", line 1599, in forward_post\n",
      "    hidden_states, cross_attn_weights = self.cross_attn(\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1167, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\", line 5161, in multi_head_attention_forward\n",
      "    attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
      "  File \"c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\", line 1841, in softmax\n",
      "    ret = input.softmax(dim)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 8.00 GiB total capacity; 7.13 GiB already allocated; 0 bytes free; 7.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python train.py --backbone facebook/mask2former-swin-tiny-ade-semantic --batch 4 --imgsz 512 512 --lr 0.0001 --epochs 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --model outputs/model_iou/ --batch 16 --imgsz 320 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_image.py --model outputs/model_iou/ --input input/road_seg/test/images/ --imgsz 512 512"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
