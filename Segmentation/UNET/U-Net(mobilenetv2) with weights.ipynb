{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.utils import normalize\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "SIZE_X = 128\n",
    "SIZE_Y = 128\n",
    "n_channels = 3\n",
    "n_classes = 10\n",
    "Batch_size = 16\n",
    "EPOCHS = 100\n",
    "MODEL = 'unet'\n",
    "BACKBONE = 'mobilenetv2'\n",
    "encoder_weights = 'imagenet'\n",
    "activation = 'softmax'\n",
    "parent_directory = r'E:\\Segmentation\\datasets\\FloodNet-Supervised_v1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(parent_directory, split='train'):\n",
    "    images = []\n",
    "    masks = [] \n",
    "\n",
    "    image_folder_path = f'{parent_directory}\\{split}\\{split}-org-img'\n",
    "    for img_path in tqdm(glob.glob(os.path.join(image_folder_path, \"*.jpg\"))):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "        images.append(img)\n",
    "\n",
    "        mask_path = (img_path.replace('org', 'label')).replace('jpg', 'png')\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)       \n",
    "        mask = cv2.resize(mask, (SIZE_Y, SIZE_X))\n",
    "        masks.append(mask)\n",
    "                \n",
    "    images = np.array(images)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    print(f'{split.upper()}: Images loaded: {images.shape[0]}')\n",
    "    print(f'{split.upper()}: Masks loaded: {masks.shape[0]}')\n",
    "\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(images, masks, unet_preporcessing):\n",
    "    if unet_preporcessing:\n",
    "        images = unet_preporcessing(images)\n",
    "    else:\n",
    "        images = normalize(images, axis=1)\n",
    "        \n",
    "    masks = np.expand_dims(masks, axis=-1)\n",
    "    masks = to_categorical(masks, num_classes=n_classes)\n",
    "    masks = masks.reshape((masks.shape[0], masks.shape[1], masks.shape[2], n_classes))\n",
    "\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_with_masks(image, mask, predicted):\n",
    "    \n",
    "    class_map= {'Background':0, 'Building-flooded':1, 'Building-non-flooded':2, 'Road-flooded':3, 'Road-non-flooded':4, 'Water':5, 'Tree':6, 'Vehicle':7, 'Pool':8, 'Grass':9}\n",
    "\n",
    "    color_map = {\n",
    "        \"Background\": [0, 0, 0],\n",
    "        \"Building-flooded\": [255, 0, 0],\n",
    "        \"Building-non-flooded\": [0, 255, 0],\n",
    "        \"Road-flooded\": [0, 255, 120],\n",
    "        \"Road-non-flooded\": [0, 0, 255],\n",
    "        \"Water\": [255, 0, 255],\n",
    "        \"Tree\": [70, 70, 70],\n",
    "        \"Vehicle\": [102, 102, 156],\n",
    "        \"Pool\": [190, 153, 153],\n",
    "        \"Grass\": [180, 165, 180]\n",
    "    }\n",
    "\n",
    "    handles = [\n",
    "        Rectangle((0, 0), 1, 1, color=np.array(c)/255) for n, c in color_map.items()\n",
    "    ]\n",
    "    labels = [n for n, c in color_map.items()]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    plt.title('Image')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    mask_colored = np.zeros_like(image, dtype=np.uint8)\n",
    "    for class_name, class_idx in class_map.items():\n",
    "        color = color_map[class_name]\n",
    "        mask_indices = np.where(mask == class_idx)\n",
    "        mask_colored[mask_indices[0], mask_indices[1], :] = color\n",
    "    plt.imshow(cv2.cvtColor(mask_colored, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Ground-Truth Mask')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    mask_colored1 = np.zeros_like(image, dtype=np.uint8)\n",
    "    for class_name1, class_idx1 in class_map.items():\n",
    "        color1 = color_map[class_name1]\n",
    "        mask_indices1 = np.where(predicted == class_idx1)\n",
    "        mask_colored1[mask_indices1[0], mask_indices1[1], :] = color1\n",
    "    plt.imshow(cv2.cvtColor(mask_colored1, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Predicted Mask')\n",
    "\n",
    "    plt.legend(handles, labels, bbox_to_anchor =(-0.8,-0.5), loc='lower center', ncol=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the images\n",
    "train_images, train_masks = load_images(parent_directory, split='train')\n",
    "val_images, val_masks = load_images(parent_directory, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "    'train_images': train_images,\n",
    "    'train_masks': train_masks,\n",
    "    'val_images': val_images,\n",
    "    'val_masks': val_masks\n",
    "}\n",
    "\n",
    "pickle_file_path = 'dataset/train_and_val_data.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(data_to_save, pickle_file)\n",
    "\n",
    "print(f'Data saved to {pickle_file_path}')\n",
    "\n",
    "print('Shapes of loaded data:')\n",
    "print('Train Images:', train_images.shape)\n",
    "print('Train Masks:', train_masks.shape)\n",
    "print('Val Images:', val_images.shape)\n",
    "print('Val Masks:', val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = '../dataset/train_and_val_data.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as pickle_file:\n",
    "    loaded_data = pickle.load(pickle_file)\n",
    "\n",
    "train_images = loaded_data['train_images']\n",
    "train_masks = loaded_data['train_masks']\n",
    "val_images = loaded_data['val_images']\n",
    "val_masks = loaded_data['val_masks']\n",
    "\n",
    "print('Shapes of loaded data:')\n",
    "print('Train Images:', train_images.shape)\n",
    "print('Train Masks:', train_masks.shape)\n",
    "print('Val Images:', val_images.shape)\n",
    "print('Val Masks:', val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "train_images, train_masks = preprocess_data(train_images, train_masks, preprocess_input)\n",
    "val_images, val_masks = preprocess_data(val_images, val_masks, preprocess_input)\n",
    "\n",
    "print('Shapes of loaded data:')\n",
    "print('Train Images:', train_images.shape)\n",
    "print('Train Masks:', train_masks.shape)\n",
    "print('Val Images:', val_images.shape)\n",
    "print('Val Masks:', val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 'unet':\n",
    "    model = sm.Unet(BACKBONE, input_shape=(SIZE_X, SIZE_Y, n_channels), classes=n_classes, encoder_weights=encoder_weights,  activation=activation)\n",
    "elif MODEL == 'fpn':\n",
    "    model = sm.FPN(BACKBONE, input_shape=(SIZE_X, SIZE_Y, n_channels), classes=n_classes, encoder_weights=encoder_weights,  activation=activation)\n",
    "elif MODEL == 'pspnet':\n",
    "    model = sm.PSPNet(BACKBONE, input_shape=(SIZE_X, SIZE_Y, n_channels), classes=n_classes, encoder_weights=encoder_weights,  activation=activation)\n",
    "elif MODEL == 'linknet':\n",
    "    model = sm.Linknet(BACKBONE, input_shape=(SIZE_X, SIZE_Y, n_channels), classes=n_classes, encoder_weights=encoder_weights,  activation=activation)\n",
    "else:\n",
    "    raise ValueError('Unknown model')\n",
    "\n",
    "model.compile('Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_images, y=train_masks, batch_size=Batch_size, epochs=EPOCHS, validation_data=(val_images, val_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'weights/{MODEL.upper()}_model_with_{EPOCHS}_epochs({MODEL.upper()}_{BACKBONE.capitalize()}).hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f'weights/{MODEL.upper()}_model_with_{EPOCHS}_epochs({MODEL.upper()}_{BACKBONE.capitalize()}).hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_masks = load_images(parent_directory, split='test')\n",
    "test_images_seperate = test_images\n",
    "\n",
    "test_images, test_masks = preprocess_data(test_images, test_masks, preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "    'test_images': test_images,\n",
    "    'test_masks': test_masks,\n",
    "    'test_images_seperate': test_images_seperate\n",
    "}\n",
    "\n",
    "pickle_file_path = '../dataset/test_data.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(data_to_save, pickle_file)\n",
    "\n",
    "print(f'Data saved to {pickle_file_path}')\n",
    "\n",
    "print('Shapes of loaded data:')\n",
    "print('Test Images:', test_images.shape)\n",
    "print('Test Masks:', test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = '../dataset/test_data.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as pickle_file:\n",
    "    loaded_data = pickle.load(pickle_file)\n",
    "\n",
    "test_images = loaded_data['test_images']\n",
    "test_masks = loaded_data['test_masks']\n",
    "test_images_seperate = loaded_data['test_images_seperate']\n",
    "\n",
    "test_images, test_masks = preprocess_data(test_images, test_masks, preprocess_input)\n",
    "\n",
    "print('Shapes of loaded data:')\n",
    "print('Test Images:', test_images.shape)\n",
    "print('Test Masks:', test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc = model.evaluate(test_images, test_masks, verbose=0)\n",
    "print(\"Accuracy is = \", (acc * 100.0), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import MeanIoU\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(test_images)\n",
    "predicted_masks = np.argmax(y_pred, axis=-1)\n",
    "test_masks_ = np.argmax(test_masks, axis=-1)\n",
    "\n",
    "print(predicted_masks.shape)\n",
    "print(test_masks_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Background\", \"Building Flooded\", \"Building Non-Flooded\", \"Road Flooded\", \"Road Non-Flooded\",\n",
    "               \"Water\", \"Tree\", \"Vehicle\", \"Pool\", \"Grass\"]\n",
    "\n",
    "n_classes = len(class_names)  \n",
    "IOU_keras = MeanIoU(num_classes=n_classes)\n",
    "\n",
    "IOU_keras.update_state(test_masks_, predicted_masks)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "values = np.array(IOU_keras.get_weights())\n",
    "values = values.reshape(n_classes, n_classes)\n",
    "\n",
    "class_IoUs = []\n",
    "print(\"IoU for each class:\")\n",
    "for i in range(n_classes):\n",
    "    class_IoU = values[i, i] / (np.sum(values[i, :]) + np.sum(values[:, i]) - values[i, i])\n",
    "    class_IoUs.append(class_IoU)\n",
    "    print(f\"{class_names[i]}: {class_IoU}\")\n",
    "\n",
    "class_APs = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_predicted_masks = (predicted_masks == i).astype(int)\n",
    "    class_test_masks = (test_masks_ == i).astype(int)\n",
    "    \n",
    "    class_predicted_masks_flat = class_predicted_masks.flatten().reshape(-1, 1)\n",
    "    class_test_masks_flat = class_test_masks.flatten().reshape(-1, 1)\n",
    "    \n",
    "    class_APs.append(average_precision_score(class_test_masks_flat, class_predicted_masks_flat, average='micro'))\n",
    "\n",
    "mAP_score = np.mean(class_APs)\n",
    "print(\"Overall mAP score =\", mAP_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(class_names, class_IoUs, color='blue')\n",
    "plt.title(f'IoU for Each Class using {MODEL.upper()} with {BACKBONE.capitalize()} Backbone ({EPOCHS} Epochs)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('IoU')\n",
    "plt.ylim(0, 1) \n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "\n",
    "for bar, value in zip(bars, class_IoUs):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2 - 0.1, bar.get_height() + 0.02, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_function():\n",
    "    image_id = np.random.randint(0, len(test_images))\n",
    "    image = test_images_seperate[image_id]\n",
    "    ground_truth_mask = test_masks_[image_id]\n",
    "    predicted_mask = predicted_masks[image_id]\n",
    "\n",
    "    print(f'Image Shape: {image.shape}')\n",
    "    print(f'Ground Truth Mask Shape: {ground_truth_mask.shape}')\n",
    "    print(f'Predicted Mask Shape: {predicted_mask.shape}')\n",
    "\n",
    "    display_images_with_masks(image, ground_truth_mask, predicted_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
