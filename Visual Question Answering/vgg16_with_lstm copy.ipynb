{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r'E:\\VQA\\floodnet\\Images'\n",
    "training_questions_path = r'E:\\VQA\\floodnet\\Questions\\Training Question.json'\n",
    "validation_questions_path = r'E:\\VQA\\floodnet\\Questions\\Valid Question.json'\n",
    "test_questions_path = r'E:\\VQA\\floodnet\\Questions\\Test_Question.json'\n",
    "\n",
    "def load_questions_and_images(json_file_path, image_dir, split='Train_Image'):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    questions = []\n",
    "    answers = []\n",
    "    image_paths = []\n",
    "    question_types = []\n",
    "\n",
    "    for item in data:\n",
    "        questions.append(data[item]['Question'])\n",
    "        answers.append(data[item]['Ground_Truth'])\n",
    "        question_types.append(data[item]['Question_Type'])\n",
    "        image_paths.append(os.path.join(image_dir, split, data[item]['Image_ID']))\n",
    "\n",
    "    return questions, answers, question_types, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>question_types</th>\n",
       "      <th>image_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the overall condition of the given image?</td>\n",
       "      <td>flooded</td>\n",
       "      <td>Condition_Recognition</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10165.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the overall condition of the given image?</td>\n",
       "      <td>flooded</td>\n",
       "      <td>Condition_Recognition</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10166.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the overall condition of the given image?</td>\n",
       "      <td>non flooded</td>\n",
       "      <td>Condition_Recognition</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10168.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many non flooded buildings can be seen in ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Complex_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10168.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many buildings can be seen in the image?</td>\n",
       "      <td>3</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10168.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions      answers  \\\n",
       "0  What is the overall condition of the given image?      flooded   \n",
       "1  What is the overall condition of the given image?      flooded   \n",
       "2  What is the overall condition of the given image?  non flooded   \n",
       "3  How many non flooded buildings can be seen in ...            3   \n",
       "4       How many buildings can be seen in the image?            3   \n",
       "\n",
       "          question_types                                   image_paths  \n",
       "0  Condition_Recognition  E:\\VQA\\floodnet\\Images\\Train_Image\\10165.JPG  \n",
       "1  Condition_Recognition  E:\\VQA\\floodnet\\Images\\Train_Image\\10166.JPG  \n",
       "2  Condition_Recognition  E:\\VQA\\floodnet\\Images\\Train_Image\\10168.JPG  \n",
       "3       Complex_Counting  E:\\VQA\\floodnet\\Images\\Train_Image\\10168.JPG  \n",
       "4        Simple_Counting  E:\\VQA\\floodnet\\Images\\Train_Image\\10168.JPG  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming load_questions_and_images function loads data correctly\n",
    "questions, answers, Question_Types, image_paths = load_questions_and_images(training_questions_path, image_dir)\n",
    "\n",
    "df = pd.DataFrame({'questions': questions, 'answers': answers, 'question_types': Question_Types, 'image_paths': image_paths})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>question_types</th>\n",
       "      <th>image_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many buildings can be seen in the image?</td>\n",
       "      <td>3</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10168.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How many buildings can be seen in this image?</td>\n",
       "      <td>4</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10170.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How many buildings are in this image?</td>\n",
       "      <td>4</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10171.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How many buildings can be seen in this image?</td>\n",
       "      <td>7</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10172.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How many buildings can be seen in the image?</td>\n",
       "      <td>1</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\10175.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        questions answers   question_types  \\\n",
       "4    How many buildings can be seen in the image?       3  Simple_Counting   \n",
       "6   How many buildings can be seen in this image?       4  Simple_Counting   \n",
       "13          How many buildings are in this image?       4  Simple_Counting   \n",
       "19  How many buildings can be seen in this image?       7  Simple_Counting   \n",
       "23   How many buildings can be seen in the image?       1  Simple_Counting   \n",
       "\n",
       "                                     image_paths  \n",
       "4   E:\\VQA\\floodnet\\Images\\Train_Image\\10168.JPG  \n",
       "6   E:\\VQA\\floodnet\\Images\\Train_Image\\10170.JPG  \n",
       "13  E:\\VQA\\floodnet\\Images\\Train_Image\\10171.JPG  \n",
       "19  E:\\VQA\\floodnet\\Images\\Train_Image\\10172.JPG  \n",
       "23  E:\\VQA\\floodnet\\Images\\Train_Image\\10175.JPG  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['question_types'] == 'Simple_Counting']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into training, validation and testing (70-15-15)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>question_types</th>\n",
       "      <th>image_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>How many buildings can be seen in this image?</td>\n",
       "      <td>3</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\7345.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>How many buildings are in the image?</td>\n",
       "      <td>5</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\6528.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>How many buildings can be seen in the image?</td>\n",
       "      <td>2</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\9073.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>How many buildings can be seen in this image?</td>\n",
       "      <td>4</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\9091.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>How many buildings can be seen in the image?</td>\n",
       "      <td>5</td>\n",
       "      <td>Simple_Counting</td>\n",
       "      <td>E:\\VQA\\floodnet\\Images\\Train_Image\\6854.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          questions answers   question_types  \\\n",
       "2085  How many buildings can be seen in this image?       3  Simple_Counting   \n",
       "472            How many buildings are in the image?       5  Simple_Counting   \n",
       "4198   How many buildings can be seen in the image?       2  Simple_Counting   \n",
       "4279  How many buildings can be seen in this image?       4  Simple_Counting   \n",
       "1131   How many buildings can be seen in the image?       5  Simple_Counting   \n",
       "\n",
       "                                      image_paths  \n",
       "2085  E:\\VQA\\floodnet\\Images\\Train_Image\\7345.JPG  \n",
       "472   E:\\VQA\\floodnet\\Images\\Train_Image\\6528.JPG  \n",
       "4198  E:\\VQA\\floodnet\\Images\\Train_Image\\9073.JPG  \n",
       "4279  E:\\VQA\\floodnet\\Images\\Train_Image\\9091.JPG  \n",
       "1131  E:\\VQA\\floodnet\\Images\\Train_Image\\6854.JPG  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Number of Simple Counting Questions: 445\n",
      "Validation Number of Simple Counting Questions: 95\n",
      "Test Number of Simple Counting Questions: 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Number of Simple Counting Questions:\", len(train_df))\n",
    "print(\"Validation Number of Simple Counting Questions:\", len(val_df))\n",
    "print(\"Test Number of Simple Counting Questions:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodNetVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (DataFrame): Pandas DataFrame containing the data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.dataframe.iloc[idx, 3] \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        question = self.dataframe.iloc[idx, 0]  \n",
    "        answer = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image, 'question': question, 'answer': answer}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = FloodNetVQADataset(train_df, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = FloodNetVQADataset(val_df, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = FloodNetVQADataset(test_df, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Images Shape: torch.Size([32, 3, 224, 224])\n",
      "Questions Shape: ['How many buildings are in the image?', 'How many buildings are in this image?', 'How many buildings can be seen in this image?', 'How many buildings are in the image?', 'How many buildings are in the image?', 'How many buildings can be seen in this image?', 'How many buildings can be seen in the image?', 'How many buildings are in the image?', 'How many buildings are in the image?', 'How many buildings can be seen in the image?', 'How many buildings can be seen in the image?', 'How many buildings are in the image?', 'How many buildings can be seen in the image?', 'How many buildings can be seen in the image?', 'How many buildings are in this image?', 'How many buildings can be seen in the image?', 'How many buildings can be seen in the image?', 'How many buildings are in the image?', 'How many buildings are in the image?', 'How many buildings can be seen in this image?', 'How many buildings can be seen in the image?', 'How many buildings are in the image?', 'How many buildings are in this image?', 'How many buildings are in this image?', 'How many buildings are in the image?', 'How many buildings are in this image?', 'How many buildings are in the image?', 'How many buildings are in the image?', 'How many buildings are in this image?', 'How many buildings can be seen in the image?', 'How many buildings can be seen in the image?', 'How many buildings can be seen in this image?']\n",
      "Answers Shape: tensor([ 6.,  1.,  3., 11.,  3.,  1.,  2.,  1.,  2.,  1.,  3.,  7., 21.,  2.,\n",
      "         4.,  4., 10., 25.,  4.,  4.,  4.,  7.,  1.,  2.,  4.,  1., 24., 10.,\n",
      "        30.,  2.,  3.,  1.])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    images = batch['image']\n",
    "    questions = batch['question']\n",
    "    answers = batch['answer'].float()\n",
    "\n",
    "    print(\"Batch:\", i)\n",
    "    print(\"Images Shape:\", images.shape)\n",
    "    print(\"Questions Shape:\", questions)\n",
    "    print(\"Answers Shape:\", answers)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdul Manaf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "feature_extractor = resnet50(weights=True)\n",
    "feature_extractor = feature_extractor.to(device)\n",
    "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-2]) \n",
    "feature_extractor.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model = bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(images)\n",
    "        features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "        features = features.view(features.size(0), -1)\n",
    "    return features\n",
    "\n",
    "\n",
    "def text_features(questions):\n",
    "    inputs = tokenizer(questions, return_tensors='pt', padding=True, truncation=True, max_length=45)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2048 + 768, 512)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 1)  \n",
    "\n",
    "    def forward(self, img_features, ques_features):\n",
    "        img_features = img_features.view(img_features.size(0), -1)\n",
    "        ques_features = ques_features.view(ques_features.size(0), -1)\n",
    "        combined_features = torch.cat((img_features, ques_features), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_model = VQAModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vqa_model = vqa_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(vqa_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQAModel(\n",
      "  (fc1): Linear(in_features=2816, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vqa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 15.339031219482422, Val Loss: 8.820296287536621\n",
      "Epoch 2/5, Loss: 5.899046421051025, Val Loss: 20.66608238220215\n",
      "Epoch 3/5, Loss: 12.975815773010254, Val Loss: 20.170452117919922\n",
      "Epoch 4/5, Loss: 3.8404624462127686, Val Loss: 6.488979816436768\n",
      "Epoch 5/5, Loss: 6.888874053955078, Val Loss: 18.440916061401367\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        images = batch['image'].to(device)\n",
    "        questions = batch['question']  \n",
    "        answers = batch['answer'].float().to(device)\n",
    "\n",
    "        img_features = extract_features(images)\n",
    "        ques_features = text_features(questions)\n",
    "        counts = answers.view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = vqa_model(img_features, ques_features)\n",
    "        loss = criterion(outputs, counts)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #validation loss\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            images = val_batch['image'].to(device)\n",
    "            questions = val_batch['question']\n",
    "            answers = val_batch['answer'].float().to(device)\n",
    "\n",
    "            img_features = extract_features(images)\n",
    "            ques_features = text_features(questions)\n",
    "            counts = answers.view(-1, 1)\n",
    "\n",
    "            outputs = vqa_model(img_features, ques_features)\n",
    "            val_loss = criterion(outputs, counts)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 9.134334564208984\n"
     ]
    }
   ],
   "source": [
    "#test loss\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_dataloader:\n",
    "        images = test_batch['image'].to(device)\n",
    "        questions = test_batch['question']\n",
    "        answers = test_batch['answer'].float().to(device)\n",
    "\n",
    "        img_features = extract_features(images)\n",
    "        ques_features = text_features(questions)\n",
    "        counts = answers.view(-1, 1)\n",
    "\n",
    "        outputs = vqa_model(img_features, ques_features)\n",
    "        test_loss = criterion(outputs, counts)\n",
    "\n",
    "print(f'Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 8\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "def predict(image_path, question):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    img_features = extract_features(image)\n",
    "\n",
    "    ques_features = text_features([question])\n",
    "    output = vqa_model(img_features, ques_features)\n",
    "    return output.item()\n",
    "\n",
    "image_path = r\"E:\\VQA\\floodnet\\Images\\Train_Image\\6693.JPG\"\n",
    "question = 'How many people are there in the image?'\n",
    "\n",
    "prediction = predict(image_path, question)\n",
    "prediction = round(prediction)\n",
    "\n",
    "print(f'Prediction: {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
